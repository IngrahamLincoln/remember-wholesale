{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU \\\n",
    "  tiktoken==0.4.0 \\\n",
    "  openai==0.27.7 \\\n",
    "  langchain==0.0.179 \\\n",
    "  pinecone-client==2.2.1 \\\n",
    "  datasets==2.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/Jamescalam--langchain-docs-23-06-27 to file:///Users/ian/.cache/huggingface/datasets/Jamescalam___json/Jamescalam--langchain-docs-23-06-27-4631410d07444b03/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 4.68M/4.68M [00:00<00:00, 7.25MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:03<00:00,  3.01s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 211.27it/s]\n",
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to file:///Users/ian/.cache/huggingface/datasets/Jamescalam___json/Jamescalam--langchain-docs-23-06-27-4631410d07444b03/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Loading a dataset cached in a LocalFileSystem is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 2\u001b[0m docs \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39;49m\u001b[39mJamescalam/langchain-docs-23-06-27\u001b[39;49m\u001b[39m'\u001b[39;49m, split \u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m docs\n",
      "File \u001b[0;32m~/Desktop/Workspaces and coding/CS320/remember-wholesale/openai-env/lib/python3.11/site-packages/datasets/load.py:1822\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1818\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1819\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   1820\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   1821\u001b[0m )\n\u001b[0;32m-> 1822\u001b[0m ds \u001b[39m=\u001b[39m builder_instance\u001b[39m.\u001b[39;49mas_dataset(split\u001b[39m=\u001b[39;49msplit, verification_mode\u001b[39m=\u001b[39;49mverification_mode, in_memory\u001b[39m=\u001b[39;49mkeep_in_memory)\n\u001b[1;32m   1823\u001b[0m \u001b[39m# Rename and cast features to match task schema\u001b[39;00m\n\u001b[1;32m   1824\u001b[0m \u001b[39mif\u001b[39;00m task \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Workspaces and coding/CS320/remember-wholesale/openai-env/lib/python3.11/site-packages/datasets/builder.py:1128\u001b[0m, in \u001b[0;36mDatasetBuilder.as_dataset\u001b[0;34m(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\u001b[0m\n\u001b[1;32m   1126\u001b[0m is_local \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m is_remote_filesystem(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fs)\n\u001b[1;32m   1127\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_local:\n\u001b[0;32m-> 1128\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading a dataset cached in a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fs)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_dir):\n\u001b[1;32m   1130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1131\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataset \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m: could not find data in \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_dir\u001b[39m}\u001b[39;00m\u001b[39m. Please make sure to call \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1132\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbuilder.download_and_prepare(), or use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1133\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdatasets.load_dataset() before trying to access the Dataset object.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1134\u001b[0m     )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Loading a dataset cached in a LocalFileSystem is not supported."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "docs = load_dataset('Jamescalam/langchain-docs-23-06-27', split ='train')\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
