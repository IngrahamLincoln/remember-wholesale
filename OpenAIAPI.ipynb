{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in terminal\n",
    "python3 -m venv openai-env\n",
    "source openai-env/bin/activate\n",
    "pip install notebook\n",
    "python -m ipykernel install --user --name=openai-env\n",
    "switch kernal over to venv enabled python kernal\n",
    "install other needed libraries like openAI\n",
    "pip install --upgrade openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: termcolor in ./openai-env/lib/python3.11/site-packages (2.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install termcolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI \n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "   api_key = os.getenv('OPEN_API_KEY'),\n",
    " )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to have JSON output there is a way to specify that you want that.\n",
    "  response_format= { \"type\":\"json_object\" }\n",
    "but apparently you have to have the response format AND you have to put the word json in your prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "def pretty_print_message(message):\n",
    "    role_to_color = {\n",
    "        \"system\": \"red\",\n",
    "        \"user\": \"green\",\n",
    "        \"assistant\": \"blue\",\n",
    "    }\n",
    "\n",
    "    # Access the 'role' and 'content' directly from the message object\n",
    "    role = message.role\n",
    "    content = message.content\n",
    "    print(colored(f\"{role}: {content}\", role_to_color[role]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_completion_message(message):\n",
    "    # Assuming 'message' is an instance of 'ChatCompletionMessage'\n",
    "    return f\"\"\"ChatCompletionMessage:\n",
    "    Content: {message.content}\n",
    "    Role: {message.role}\n",
    "    Function Call: {message.function_call}\n",
    "    Tool Calls: {message.tool_calls}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8IhXBytPvOWbxYgBS8wJrtaYMOAXy', choices=[Choice(finish_reason='length', index=0, message=ChatCompletionMessage(content='Certainly! Testing new assistant GPT models from OpenAI involves a variety of strategies to ensure the model', role='assistant', function_call=None, tool_calls=None))], created=1699467941, model='gpt-4-1106-preview', object='chat.completion', system_fingerprint='fp_a24b4d720c', usage=CompletionUsage(completion_tokens=20, prompt_tokens=45, total_tokens=65))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"you are a personal assistant, your job is to determine whether or not a request requires you to return the time. \"}, #please respond in JSON specify keys too\n",
    "    {\"role\": \"user\", \"content\": \"Help me to brainstorm ways of testing the new assistant GPT models from openAI.\"}\n",
    "  ],\n",
    "  temperature = 0.7,\n",
    "  #frequency_penalty = 0,\n",
    "  #logit_bias = None,\n",
    "  max_tokens = 20,\n",
    "  n = 1,\n",
    "  #stream = True, I need to figure out how to print when streaming. low priority rn\n",
    "  presence_penalty = 0,\n",
    "  #response_format= { \"type\":\"json_object\" }\n",
    ")\n",
    "\n",
    "\n",
    "# Ensure the response is properly structured before passing it to the function\n",
    "\n",
    "#print(response.choices[0].message)\n",
    "print(response)\n",
    "# if response.choices:\n",
    "#     # It looks like each choice contains only one message rather than a list of messages\n",
    "#     for choice in response.choices:\n",
    "#         pretty_print_message(choice.message)\n",
    "# else:\n",
    "#     print(\"No choices in response.\")\n",
    "\n",
    "# formatted_message = format_chat_completion_message(response)\n",
    "# print(formatted_message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With   response_format= { \"type\":\"json_object\" }\n",
    "ChatCompletion(id='chatcmpl-8IOYsF5XupjXqntyQN6xoB5j0t1it', choices=[Choice(finish_reason='length', index=0, message=ChatCompletionMessage(content='{\\n  \"TestingStrategies\": [\\n    {\\n      \"Description\": \"Functional Testing\",\\n      \"', role='assistant', function_call=None, tool_calls=None))], created=1699395010, model='gpt-4-1106-preview', object='chat.completion', system_fingerprint='fp_a24b4d720c', usage=CompletionUsage(completion_tokens=20, prompt_tokens=48, total_tokens=68))\n",
    "\n",
    "\n",
    "without\n",
    "ChatCompletionMessage(content='Sure, I\\'d be happy to help you brainstorm ways of testing the new assistant GPT models from OpenAI. Here are a few ideas:\\n\\n1. **Test with a Variety of Prompts:** To understand how well the model responds to different types of prompts, you should test it with a wide range of text inputs. These could be questions, statements, or even stories across a range of topics. \\n\\n2. **Check for Coherence:** Check if the model\\'s responses are coherent and make sense within the context provided. For example, if you ask it about a historical event, does it provide an accurate and logically sound response?\\n\\n3. **Test for Creativity:** Try giving the model creative prompts, like asking it to write a poem or a short story. This would test the model\\'s ability to generate original and creative outputs.\\n\\n4. **Test for Consistency:** Ask a sequence of related questions to check the model\\'s consistency in its responses. For example, if you ask about the weather in New York today and follow up with \"What should I wear?\", the model should respond appropriately.\\n\\n5. **Test Edge Cases:** Try to find the model\\'s limitations by testing edge cases. This might include complex mathematical problems, obscure trivia, or nonsensical prompts.\\n\\n6. **Test for Sensitivity to Small Changes:** Try altering your prompts slightly to see how much the model\\'s answers change. This could help assess how sensitive it is to small changes in input.\\n\\n7. **Test Language Skills:** Test the model\\'s ability to understand and generate text in different languages. You can also test its understanding of slang, idioms, or cultural references.\\n\\n8. **Bias Detection:** Test the model with different scenarios to understand if it\\'s biased in any way. This can be towards a particular gender, race, or ethnicity.\\n\\n9. **Test for Appropriateness:** You should also assess if the model is able to filter out inappropriate or harmful responses.\\n\\n10. **Evaluate Problem-Solving Skills:** Give the model some problems to solve. These could include logic puzzles, coding problems, or riddles.\\n\\nRemember, the more diverse and comprehensive your testing, the better you\\'ll understand the model\\'s strengths and weaknesses.', role='assistant', function_call=None, tool_calls=None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
