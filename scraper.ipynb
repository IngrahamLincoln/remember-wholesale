{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "import time \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully fetched the page!\n",
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-zebra-design-disabled vector-feature-custom-font-size-clientpref-0 vector-feature-client-preferences-disabled vector-feature-typography-survey-disabled vector-toc-available\" dir=\"ltr\" lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   Artificial intelligence - Wikipedia\n",
      "  </title>\n",
      "  <script>\n",
      "   (function(){var className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-\n"
     ]
    }
   ],
   "source": [
    "# URL of the Wikipedia page for \"Artificial Intelligence\"\n",
    "url = 'https://en.wikipedia.org/wiki/Artificial_intelligence'\n",
    "\n",
    "# Make an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"successfully fetched the page!\")\n",
    "    html_content = response.text\n",
    "else:\n",
    "    print(f\"Failed to retrive the page. Status code: {response.status_code}\")\n",
    "    html_content = \"\"\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Print out the parsed HTML\n",
    "print(soup.prettify()[:1000])  # Print the first 1000 characters to check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all <a> tags in the HTML\n",
    "links = soup.find_all('a')\n",
    "\n",
    "# Filter out and print the URLs of AI-related articles\n",
    "for link in links:\n",
    "    href = link.get('href')\n",
    "    if href and '/wiki/' in href:  # Checks if it's a link to another Wikipedia article\n",
    "        print(f\"https://en.wikipedia.org{href}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty set to store unique URLs\n",
    "unique_urls = set()\n",
    "\n",
    "# Find all <a> tags and add their href attributes to the set\n",
    "for link in soup.find_all('a'):\n",
    "    href = link.get('href')\n",
    "    if href and '/wiki/' in href:\n",
    "        full_url = f\"https://en.wikipedia.org{href}\"\n",
    "        unique_urls.add(full_url)\n",
    "\n",
    "import time\n",
    "\n",
    "base_url = 'https://en.wikipedia.org'\n",
    "\n",
    "urlCount = 0\n",
    "while (urlCount <= 2):\n",
    "    for url in unique_urls:\n",
    "        if url.startswith('/wiki/'):\n",
    "            url= base_url + url\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "        \n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Parse the HTML content\n",
    "                article_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # Extract the title of the article\n",
    "                title = article_soup.find('h1').text\n",
    "                \n",
    "                # Extract the summary\n",
    "                # Wikipedia summaries are typically in the first few paragraphs after the title\n",
    "                paragraphs = article_soup.find_all('p')\n",
    "                summary = ' '.join(paragraph.text for paragraph in paragraphs[:3])  # Adjust the range as needed\n",
    "\n",
    "                # Print the title and the summary\n",
    "                print(f\"Title: {title}\")\n",
    "                print(f\"Summary: {summary}\")\n",
    "                print(\"\\n-----------------------------------\\n\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
    "            \n",
    "            urlCount += 1\n",
    "            time.sleep(1)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"error fetching {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"terrible ideas that just might work\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
